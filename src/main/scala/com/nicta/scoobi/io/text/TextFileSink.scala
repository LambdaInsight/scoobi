/**
 * Copyright 2011,2012 National ICT Australia Limited
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.nicta
package scoobi
package io
package text

import core._
import org.apache.hadoop.mapred.InvalidJobConfException
import partition._
import org.apache.hadoop.io.NullWritable
import org.apache.commons.logging.LogFactory
import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat, TextOutputFormat, FileOutputCommitter}
import org.apache.hadoop.mapreduce._
import impl.io.Files
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.io.compress.CompressionCodec
import org.apache.hadoop.io.SequenceFile.CompressionType
import impl.ScoobiConfigurationImpl

import scala.Some


case class TextFileSink[A : Manifest](path: String, overwrite: Boolean = false, check: Sink.OutputCheck = Sink.defaultOutputCheck, compression: Option[Compression] = None) extends DataSink[NullWritable, A, A] {
  private lazy val logger = LogFactory.getLog("scoobi.TextOutput")

  private val output = new Path(path)

  def outputFormat(implicit sc: ScoobiConfiguration): Class[_ <: OutputFormat[NullWritable, A]] =
    if (overwrite) classOf[OverwritableTextOutputFormat[NullWritable, A]]
    else           classOf[TextOutputFormat[NullWritable, A]]

  def outputKeyClass(implicit sc: ScoobiConfiguration) = classOf[NullWritable]
  def outputValueClass(implicit sc: ScoobiConfiguration) = TextFileSink.runtimeClass[A]
  def outputCheck(implicit sc: ScoobiConfiguration) {
    check(output, overwrite, sc)
  }

  def outputPath(implicit sc: ScoobiConfiguration) = Some(output)
  def outputConfigure(job: Job)(implicit sc: ScoobiConfiguration) {}

  override def outputSetup(implicit sc: ScoobiConfiguration) {
    super.outputSetup(sc)

    if (Files.pathExists(output)(sc.configuration) && overwrite) {
      logger.info("Deleting the pre-existing output path: " + output.toUri.toASCIIString)
      Files.deletePath(output)(sc.configuration)
    }
  }

  lazy val outputConverter = new OutputConverter[NullWritable, A, A] {
    def toKeyValue(x: A)(implicit configuration: Configuration) = (NullWritable.get, x)
  }

  def compressWith(codec: CompressionCodec, compressionType: CompressionType = CompressionType.BLOCK) = copy(compression = Some(Compression(codec, compressionType)))

  override def toString = getClass.getSimpleName+": "+outputPath(new ScoobiConfigurationImpl).getOrElse("none")
}

object TextFileSink {
  def runtimeClass[A : Manifest] = implicitly[Manifest[A]] match {
    case Manifest.Boolean => classOf[java.lang.Boolean].asInstanceOf[Class[A]]
    case Manifest.Char    => classOf[java.lang.Character].asInstanceOf[Class[A]]
    case Manifest.Short   => classOf[java.lang.Short].asInstanceOf[Class[A]]
    case Manifest.Int     => classOf[java.lang.Integer].asInstanceOf[Class[A]]
    case Manifest.Long    => classOf[java.lang.Long].asInstanceOf[Class[A]]
    case Manifest.Float   => classOf[java.lang.Float].asInstanceOf[Class[A]]
    case Manifest.Double  => classOf[java.lang.Double].asInstanceOf[Class[A]]
    case Manifest.Byte    => classOf[java.lang.Byte].asInstanceOf[Class[A]]
    case mf               => mf.runtimeClass.asInstanceOf[Class[A]]
  }
}

/**
 * This format creates a new text record writer for each different path that's generated by the partition function
 * Each record writer defines a specific OutputCommitter that will define a different work directory for a given key.
 *
 * All the generated paths will be created under temporary dir/sink id in order to collect them
 * more rapidly with just a rename of directories (see OutputChannel)
 */
class PartitionedTextOutputFormat[P, K, V] extends PartitionedOutputFormat[P, K, V] {

  protected def getBaseRecordWriter(context: TaskAttemptContext, path: Path): RecordWriter[K, V] =
    new TextOutputFormat[K, V] {
      override def getOutputCommitter(context: TaskAttemptContext) = new FileOutputCommitter(path, context) {
        // we need to use path as the work path for the record writers because it
        // already contains the work directories
        override def getWorkPath = path
      }

      /**
       * override this method to give an output name without a directory
       * so that files are written directly as year=2014/month=01/day=23/out-xxxx instead of
       * year=2014/month=01/day=23/ch3-4/out-xxxx
       *
       * Because ch3-4/out is the default basename configured in ChannelOutputFormat
       */
      override def getDefaultWorkFile(context: TaskAttemptContext, extension: String): Path = {
        val committer: FileOutputCommitter = getOutputCommitter(context).asInstanceOf[FileOutputCommitter]
        new Path(committer.getWorkPath, FileOutputFormat.getUniqueFile(context, "out", extension))
      }
    }.getRecordWriter(context)
}

class OverwritableTextOutputFormat[K, V] extends TextOutputFormat[K, V] {
  override def checkOutputSpecs(job: JobContext) {
    val outDir: Path = FileOutputFormat.getOutputPath(job)
    if (outDir == null) {
      throw new InvalidJobConfException("Output directory not set.")
    }
  }
}

